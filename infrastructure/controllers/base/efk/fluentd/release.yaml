apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: fluentd
  namespace: logging
spec:
  interval: 15m
  chart:
    spec:
      chart: fluentd
      version: "0.3.9"
      sourceRef:
        kind: HelmRepository
        name: fluent
        namespace: logging
  install:
    remediation:
      retries: 3
  values:
    kind: DaemonSet

    # Disable Pod Security Policy
    rbac:
      create: true
      pspEnabled: false

    podSecurityPolicy:
      enabled: false

    # Skip health probes
    livenessProbe: null
    readinessProbe: null

    # Add an explicit nameOverride
    nameOverride: fluentd-logging

    # Basic resources
    resources:
      requests:
        cpu: 50m
        memory: 200Mi
      limits:
        cpu: 200m
        memory: 512Mi

    # Permission to access logs
    securityContext:
      privileged: true
      runAsUser: 0

    # CRITICAL: Add tolerations to run on all nodes
    tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule

    # CRITICAL: Add volume mounts to access logs
    extraVolumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers

    extraVolumeMounts:
      - name: varlog
        mountPath: /var/log
      - name: varlibdockercontainers
        mountPath: /var/lib/docker/containers
        readOnly: true

    # Add plugins for better log handling
    plugins:
      - fluent-plugin-rewrite-tag-filter
      - fluent-plugin-parser
      - fluent-plugin-concat

    # Custom Fluentd config for separate indices
    configMapConfigs:
      fluent.conf: |-
        # System config to handle long logs
        <system>
          log_level info
          <log>
            format json
          </log>
        </system>

        # Container logs source
        <source>
          @type tail
          path /var/log/containers/*.log
          pos_file /var/log/fluentd-containers.log.pos
          tag kubernetes.*
          read_from_head true
          <parse>
            @type json
            json_parser json
            time_format %Y-%m-%dT%H:%M:%S.%NZ
            time_key time
            keep_time_key true
          </parse>
        </source>

        # Add Kubernetes metadata
        <filter kubernetes.**>
          @type kubernetes_metadata
          kubernetes_url https://#{ENV['KUBERNETES_SERVICE_HOST']}:#{ENV['KUBERNETES_SERVICE_PORT']}
          bearer_token_file /var/run/secrets/kubernetes.io/serviceaccount/token
          ca_file /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          skip_labels false
          skip_annotations false
        </filter>

        # Truncate extremely long log lines
        <filter kubernetes.**>
          @type record_transformer
          enable_ruby true
          <record>
            message ${record["message"].to_s.length > 32768 ? record["message"].to_s[0..32767] + "... (truncated)" : record["message"]}
          </record>
        </filter>

        # Route logs by namespace more reliably
        <match kubernetes.**>
          @type rewrite_tag_filter
          <rule>
            key $.kubernetes.namespace_name
            pattern ^(linkding)$
            tag linkding.${tag}
          </rule>
          <rule>
            key $.kubernetes.namespace_name
            pattern ^(audiobookshelf)$
            tag audiobookshelf.${tag}
          </rule>
          <rule>
            key $.kubernetes.host
            pattern ^(.+)$
            tag kube.$1.${tag}
          </rule>
          <rule>
            key message
            pattern .*
            tag other.${tag}
          </rule>
        </match>

        # Send linkding logs to dedicated index
        <match linkding.**>
          @type elasticsearch
          host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
          port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
          scheme "#{ENV['FLUENT_ELASTICSEARCH_SCHEME']}"
          user "#{ENV['FLUENT_ELASTICSEARCH_USER']}"
          password "#{ENV['FLUENT_ELASTICSEARCH_PASSWORD']}"
          index_name linkding-%{+YYYY.MM.dd}
          include_timestamp true
          buffer_chunk_limit 16M
        </match>

        # Send audiobookshelf logs to dedicated index
        <match audiobookshelf.**>
          @type elasticsearch
          host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
          port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
          scheme "#{ENV['FLUENT_ELASTICSEARCH_SCHEME']}"
          user "#{ENV['FLUENT_ELASTICSEARCH_USER']}"
          password "#{ENV['FLUENT_ELASTICSEARCH_PASSWORD']}"
          index_name audiobookshelf-%{+YYYY.MM.dd}
          include_timestamp true
          buffer_chunk_limit 16M
        </match>

        # Send master node logs to dedicated index
        <match kube.master.**>
          @type elasticsearch
          host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
          port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
          scheme "#{ENV['FLUENT_ELASTICSEARCH_SCHEME']}"
          user "#{ENV['FLUENT_ELASTICSEARCH_USER']}"
          password "#{ENV['FLUENT_ELASTICSEARCH_PASSWORD']}"
          index_name master-node-%{+YYYY.MM.dd}
          include_timestamp true
          buffer_chunk_limit 16M
        </match>

        # Send p2 node logs to dedicated index
        <match kube.p2.**>
          @type elasticsearch
          host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
          port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
          scheme "#{ENV['FLUENT_ELASTICSEARCH_SCHEME']}"
          user "#{ENV['FLUENT_ELASTICSEARCH_USER']}"
          password "#{ENV['FLUENT_ELASTICSEARCH_PASSWORD']}"
          index_name worker-node-%{+YYYY.MM.dd}
          include_timestamp true
          buffer_chunk_limit 16M
        </match>

        # Catch all other logs
        <match **>
          @type elasticsearch
          host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
          port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
          scheme "#{ENV['FLUENT_ELASTICSEARCH_SCHEME']}"
          user "#{ENV['FLUENT_ELASTICSEARCH_USER']}"
          password "#{ENV['FLUENT_ELASTICSEARCH_PASSWORD']}"
          index_name other-logs-%{+YYYY.MM.dd}
          include_timestamp true
          buffer_chunk_limit 16M
        </match>

    # Environment variables for Elasticsearch connection
    env:
      - name: FLUENT_ELASTICSEARCH_HOST
        value: "elasticsearch-master"
      - name: FLUENT_ELASTICSEARCH_PORT
        value: "9200"
      - name: FLUENT_ELASTICSEARCH_SCHEME
        value: "http"
      - name: FLUENT_ELASTICSEARCH_USER
        valueFrom:
          secretKeyRef:
            name: elasticsearch-credentials
            key: username
      - name: FLUENT_ELASTICSEARCH_PASSWORD
        valueFrom:
          secretKeyRef:
            name: elasticsearch-credentials
            key: password
      # Prevent buffer overflow with large logs
      - name: FLUENT_CONTAINER_TAIL_EXCLUDE_PATH
        value: '["/var/log/containers/fluentd-*"]'
      - name: FLUENT_CONTAINER_TAIL_PARSER_TYPE
        value: "json"
      # Increase buffer limits for large logs
      - name: FLUENT_ELASTICSEARCH_BUFFER_CHUNK_LIMIT_SIZE
        value: "16M"
      # Add plugin for rewrite_tag_filter
      - name: FLUENTD_SYSTEMD_CONF
        value: "disable"
