apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-main-config
  namespace: fluent
data:
  fluent.conf: |-
    # Include conf files in config.d directory
    @include config.d/*.conf
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-extra-files
  namespace: fluent
data:
  01_sources.conf: |-
    ## FluentD config file
    ## ✅ AKTIV: Sources - logs von Filebeat
    <source>
        @type beats
        @label @BEATS
        bind "#{ENV['FLUENTD_BEATS_BIND'] || '0.0.0.0'}"
        port "#{ENV['FLUENTD_BEATS_PORT'] || '5044'}"
        metadata_as_tag true
    </source>
    
    ## ✅ KOMMENTIERT: Sources - logs von Fluent Bit forwarders
    ## Für Fluent Bit aktivieren, für Filebeat deaktivieren
    # <source>
    #     @type forward
    #     @label @FORWARD
    #     bind "#{ENV['FLUENTD_FORWARD_BIND'] || '0.0.0.0'}"
    #     port "#{ENV['FLUENTD_FORWARD_PORT'] || '24224'}"
    #     # Enabling TLS (commented out for initial setup)
    #     # <transport tls>
    #     #     cert_path /etc/fluent/certs/tls.crt
    #     #     private_key_path /etc/fluent/certs/tls.key
    #     # </transport>
    #     # Enabling access security
    #     <security>
    #         self_hostname "#{ENV['FLUENTD_FORWARD_SEC_SELFHOSTNAME'] || 'fluentd-aggregator'}"
    #         shared_key "#{ENV['FLUENT_AGGREGATOR_SHARED_KEY'] || 'sharedkey'}"
    #     </security>
    # </source>
    
    ## Enable Prometheus end point
    <source>
        @type prometheus
        @id in_prometheus
        bind "0.0.0.0"
        port 24231
        metrics_path "/metrics"
    </source>
    <source>
        @type prometheus_monitor
        @id in_prometheus_monitor
    </source>
    <source>
        @type prometheus_output_monitor
        @id in_prometheus_output_monitor
    </source>
  
  02_filters.conf: |-
    ## ✅ AKTIV: Filters für Filebeat
    <label @BEATS>
        # Parse Filebeat JSON logs
        <filter beats.**>
            @type parser
            key_name message
            reserve_data true
            remove_key_name_field true
            emit_invalid_record_to_error false
            <parse>
                @type json
                json_parser_class: json
            </parse>
        </filter>
        
        # Add cluster metadata for Beats logs
        <filter beats.**>
            @type record_transformer
            enable_ruby
            <record>
                cluster_name pi-cluster
                log_source filebeat
                timestamp ${time.strftime('%Y-%m-%dT%H:%M:%S.%3NZ', time)}
            </record>
        </filter>
        
        # Re-route beats logs to dispatch
        <match beats.**>
            @type relabel
            @label @DISPATCH
        </match>
    </label>
    
    ## ✅ KOMMENTIERT: Filters für Fluent Bit
    ## Für Fluent Bit aktivieren, für Filebeat deaktivieren
    # <label @FORWARD>
    #     # Re-route fluentd logs
    #     <match kube.var.log.containers.fluentd**>
    #         @type relabel
    #         @label @FLUENT_LOG
    #     </match>
    #     <match **>
    #         @type relabel
    #         @label @DISPATCH
    #     </match>
    # </label>
  
  03_dispatch.conf: |-
    # Discard FLUENTD LOGS
    <label @FLUENT_LOG>
      <match **>
        @type null
        @id ignore_fluent_logs
      </match>
    </label>
    
    # Dispatch logs to different destinations
    <label @DISPATCH>
        # Calculate prometheus metrics
        <filter **>
            @type prometheus
            <metric>
                name fluentd_input_status_num_records_total
                type counter
                desc The total number of incoming records
                <labels>
                tag ${tag}
                hostname ${host}
                </labels>
            </metric>
        </filter>
        # Copy log stream to different outputs
        <match **>
            @type copy
            <store>
                @type relabel
                @label @OUTPUT_ES
            </store>
        </match>
    </label>
  
  04_output.conf: |-
    # Elastic Search Output
    <label @OUTPUT_ES>
        # ✅ AKTIV: Setup index name für Filebeat
        <filter beats.**>
            @type record_transformer
            enable_ruby
            <record>
                index_name ${record['kubernetes'] && record['kubernetes']['namespace'] ? record['kubernetes']['namespace'] : 'filebeat'}
            </record>
        </filter>
        
        # ✅ KOMMENTIERT: Setup index name für Fluent Bit
        ## Für Fluent Bit aktivieren, für Filebeat deaktivieren
        # <filter kube.**>
        #     @type record_transformer
        #     enable_ruby
        #     <record>
        #         index_name ${record['namespace']}
        #     </record>
        # </filter>
        # <filter host.**>
        #     @type record_transformer
        #     enable_ruby
        #     <record>
        #         index_name "host"
        #     </record>
        # </filter>
        
        # Send received logs to elasticsearch
        <match **>
            @type elasticsearch
            @id out_es
            @log_level info
            include_tag_key true
            host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
            port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
            scheme http
            user "#{ENV['FLUENT_ELASTICSEARCH_USER'] || use_default}"
            password "#{ENV['FLUENT_ELASTICSEARCH_PASSWORD'] || use_default}"
            
            # Reload and reconnect options
            reconnect_on_error true
            reload_on_failure true
            reload_connections false
            request_timeout 15s
            log_es_400_reason true
            suppress_type_name true
            sniffer_class_name Fluent::Plugin::ElasticsearchSimpleSniffer
            
            # ✅ AKTIV: Filebeat-Format
            logstash_format false
            index_name fluentd-${index_name}
            time_key time
            include_timestamp true
            
            # ✅ KOMMENTIERT: Fluent Bit-Format
            ## Für Fluent Bit aktivieren, für Filebeat deaktivieren
            # logstash_format false
            # index_name fluentd-${index_name}
            # time_key time
            # include_timestamp true
            
            # ILM Settings - WITH ROLLOVER support
            index_date_pattern ""
            enable_ilm true
            ilm_policy_id fluentd-policy
            ilm_policy {"policy":{"phases":{"hot":{"min_age":"0ms","actions":{"rollover":{"max_size":"10gb","max_age":"7d"}}},"warm":{"min_age":"2d","actions":{"shrink":{"number_of_shards":1},"forcemerge":{"max_num_segments":1}}},"delete":{"min_age":"7d","actions":{"delete":{"delete_searchable_snapshot":true}}}}}}
            ilm_policy_overwrite true
            
            # index template
            use_legacy_template false
            template_overwrite true
            template_name fluentd-${index_name}
            template_file "/etc/fluent/template/fluentd-es-template.json"
            # Angepasst für deine 2-Node Elasticsearch
            customize_template {"<<shard>>": "1","<<replica>>": "1", "<<TAG>>":"${index_name}"}
            remove_keys index_name
            
            <buffer tag, index_name>
                flush_thread_count "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_FLUSH_THREAD_COUNT'] || '8'}"
                flush_interval "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_FLUSH_INTERVAL'] || '5s'}"
                chunk_limit_size "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_CHUNK_LIMIT_SIZE'] || '2M'}"
                queue_limit_length "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_QUEUE_LIMIT_LENGTH'] || '32'}"
                retry_max_interval "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_RETRY_MAX_INTERVAL'] || '30'}"
                retry_forever true
            </buffer>
        </match>
    </label>