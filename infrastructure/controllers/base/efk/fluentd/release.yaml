apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: logstash
  namespace: logging
spec:
  interval: 15m
  chart:
    spec:
      chart: logstash
      version: "7.17.3"
      sourceRef:
        kind: HelmRepository
        name: elastic
        namespace: logging
  install:
    timeout: 10m
    remediation:
      retries: 3
  # Force recreate instead of trying to patch an existing StatefulSet
  uninstall:
    keepHistory: false
  upgrade:
    remediation:
      retries: 3
    timeout: 10m
  values:
    replicas: 1
    image: "docker.elastic.co/logstash/logstash"
    imageTag: "7.17.3"

    # Use Deployment instead of StatefulSet - more flexible for upgrades
    kind: Deployment

    # Simple log directory structure
    persistence:
      enabled: true
      storageClassName: "local-path"
      accessMode: "ReadWriteOnce"
      size: "2Gi"

    # Resource limits to ensure it can schedule
    resources:
      requests:
        cpu: 100m
        memory: 512Mi
      limits:
        cpu: 1000m
        memory: 1Gi

    # Important for container log access
    securityContext:
      runAsUser: 0
      privileged: true

    podSecurityContext:
      runAsUser: 0
      fsGroup: 0

    # Longer timeouts for startup
    readinessProbe:
      initialDelaySeconds: 60
      periodSeconds: 10
      timeoutSeconds: 5

    livenessProbe:
      initialDelaySeconds: 60
      periodSeconds: 20
      timeoutSeconds: 5

    # Mount container logs
    extraVolumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers

    extraVolumeMounts:
      - name: varlog
        mountPath: /var/log
        readOnly: true
      - name: varlibdockercontainers
        mountPath: /var/lib/docker/containers
        readOnly: true

    # Initialize sincedb directory for file inputs
    extraInitContainers:
      - name: init-sincedb
        image: busybox
        command:
          [
            "sh",
            "-c",
            "mkdir -p /usr/share/logstash/data/sincedb && chmod -R 777 /usr/share/logstash/data",
          ]
        volumeMounts:
          - name: data
            mountPath: /usr/share/logstash/data

    # Optimized JVM settings
    extraEnv:
      - name: LS_JAVA_OPTS
        value: "-Xms256m -Xmx512m"
      - name: ELASTICSEARCH_USER
        valueFrom:
          secretKeyRef:
            name: elasticsearch-credentials
            key: username
      - name: ELASTICSEARCH_PASSWORD
        valueFrom:
          secretKeyRef:
            name: elasticsearch-credentials
            key: password

    # Basic configuration
    logstashConfig:
      logstash.yml: |
        http.host: "0.0.0.0"
        path.data: /usr/share/logstash/data
        path.logs: /usr/share/logstash/logs
        log.level: info
        pipeline.workers: 2
        pipeline.batch.size: 125

    # Simple pipeline configuration
    logstashPipeline:
      logstash.conf: |
        input {
          file {
            path => "/var/log/containers/*.log"
            exclude => ["/var/log/containers/*fluentd*", "/var/log/containers/*logstash*", "/var/log/containers/*elastic*", "/var/log/containers/*kibana*"]
            sincedb_path => "/usr/share/logstash/data/sincedb"
            start_position => "beginning"
            codec => "json"
            tags => ["kubernetes"]
          }
        }

        filter {
          # Extract pod metadata from file path
          grok {
            match => { "path" => "/var/log/containers/%{DATA:pod_name}_%{DATA:namespace}_%{GREEDYDATA:container_name}\.log" }
            remove_field => ["path"]
          }

          # Skip excessive backslashes that cause recursion
          if [log] =~ /\\{5,}/ {
            drop {}
          }

          # Basic kubernetes metadata
          mutate {
            add_field => {
              "kubernetes" => {
                "namespace" => "%{namespace}",
                "pod" => "%{pod_name}",
                "container" => "%{container_name}"
              }
            }
            remove_field => ["namespace", "pod_name", "container_name"]
          }

          # Make sure we have a timestamp
          date {
            match => ["time", "ISO8601"]
            remove_field => ["time"]
          }
        }

        output {
          elasticsearch {
            hosts => ["elasticsearch-master:9200"]
            user => "${ELASTICSEARCH_USER}"
            password => "${ELASTICSEARCH_PASSWORD}"
            index => "logs-%{+YYYY.MM.dd}"
            
            # Use simple retry policy
            retry_on_conflict => 5
            action => "index"
          }
        }
