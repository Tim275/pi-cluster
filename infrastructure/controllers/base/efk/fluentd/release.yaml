apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: fluentd
  namespace: logging
spec:
  interval: 15m
  chart:
    spec:
      chart: fluentd
      version: "0.3.9"
      sourceRef:
        kind: HelmRepository
        name: fluent
        namespace: logging
  install:
    remediation:
      retries: 3
  values:
    kind: DaemonSet

    # Disable Pod Security Policy
    rbac:
      create: true
      pspEnabled: false

    podSecurityPolicy:
      enabled: false

    # Skip health probes
    livenessProbe: null
    readinessProbe: null

    # Add an explicit nameOverride
    nameOverride: fluentd-logging

    # Basic resources
    resources:
      requests:
        cpu: 50m
        memory: 200Mi
      limits:
        cpu: 200m
        memory: 512Mi

    # Permission to access logs
    securityContext:
      privileged: true
      runAsUser: 0

    # CRITICAL: Add tolerations to run on all nodes
    tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule

    # CRITICAL: Add volume mounts to access logs
    extraVolumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers

    extraVolumeMounts:
      - name: varlog
        mountPath: /var/log
      - name: varlibdockercontainers
        mountPath: /var/lib/docker/containers
        readOnly: true

    # Add plugins for better log handling
    plugins:
      - fluent-plugin-rewrite-tag-filter
      - fluent-plugin-parser
      - fluent-plugin-concat
      - fluent-plugin-grep

    # Custom Fluentd config for separate indices
    configMapConfigs:
      fluent.conf: |-
        # System config to handle long logs
        <system>
          log_level error
          <log>
            format json
          </log>
          suppress_repeated_stacktrace true
          ignore_repeated_log_interval 60
          emit_error_log_interval 60
          suppress_config_dump true
        </system>

        # Container logs source with better error handling
        <source>
          @type tail
          path /var/log/containers/*.log
          pos_file /var/log/fluentd-containers.log.pos
          tag kubernetes.*
          read_from_head true
          skip_refresh_on_startup true
          refresh_interval 60
          read_bytes_limit_per_second 16777216  # 16MB/s
          <parse>
            @type regexp
            expression /^(?<time>.+) (?<stream>stdout|stderr) (?<logtag>[F|P]) (?<log>.*)$/
            time_format %Y-%m-%dT%H:%M:%S.%NZ
            keep_time_key true
          </parse>
        </source>

        # Drop logs with excessive backslashes completely
        <filter kubernetes.**>
          @type grep
          <exclude>
            key log
            pattern /\\{8,}/
          </exclude>
        </filter>

        # Heavily simplify logs with excessive backslashes
        <filter kubernetes.**>
          @type record_transformer
          enable_ruby true
          <record>
            message ${
              if record["log"].nil?
                record["message"] || "Empty log entry"
              elsif record["log"].to_s.include?("\\\\\\\\") || record["log"].to_s.length > 10000
                "Log entry truncated - contained excessive backslashes or was too long"
              else
                record["log"].to_s.gsub(/\\{3,}/, "[ESCAPED CONTENT]")
              end
            }
          </record>
          remove_keys log
        </filter>

        # Add Kubernetes metadata
        <filter kubernetes.**>
          @type kubernetes_metadata
          kubernetes_url https://#{ENV['KUBERNETES_SERVICE_HOST']}:#{ENV['KUBERNETES_SERVICE_PORT']}
          bearer_token_file /var/run/secrets/kubernetes.io/serviceaccount/token
          ca_file /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          skip_labels false
          skip_annotations false
        </filter>

        # Truncate extremely long log lines
        <filter kubernetes.**>
          @type record_transformer
          enable_ruby true
          <record>
            message ${record["message"].to_s.length > 8192 ? record["message"].to_s[0..8192] + "... (truncated)" : record["message"]}
          </record>
        </filter>

        # Rest of your configuration remains the same
        # Route logs by namespace more reliably
        <match kubernetes.**>
          @type rewrite_tag_filter
          <rule>
            key $.kubernetes.namespace_name
            pattern ^(linkding)$
            tag linkding.${tag}
          </rule>
          <rule>
            key $.kubernetes.namespace_name
            pattern ^(audiobookshelf)$
            tag audiobookshelf.${tag}
          </rule>
          <rule>
            key $.kubernetes.host
            pattern ^(.+)$
            tag kube.$1.${tag}
          </rule>
          <rule>
            key message
            pattern .*
            tag other.${tag}
          </rule>
        </match>

        # Elasticsearch output configuration
        <match **>
          @type elasticsearch
          host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
          port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
          scheme "#{ENV['FLUENT_ELASTICSEARCH_SCHEME']}"
          user "#{ENV['FLUENT_ELASTICSEARCH_USER']}"
          password "#{ENV['FLUENT_ELASTICSEARCH_PASSWORD']}"
          
          # Use tag-based index names for your separate namespaces
          index_name ${tag}
          
          <buffer tag>
            @type file
            path /var/log/fluentd-buffers/kubernetes.buffer
            flush_mode interval
            retry_type exponential_backoff
            flush_thread_count 2
            flush_interval 5s
            retry_forever false
            retry_max_interval 30
            chunk_limit_size 16M
            queue_limit_length 32
          </buffer>
        </match>

    # Environment variables for Elasticsearch connection
    env:
      - name: FLUENT_ELASTICSEARCH_HOST
        value: "elasticsearch-master"
      - name: FLUENT_ELASTICSEARCH_PORT
        value: "9200"
      - name: FLUENT_ELASTICSEARCH_SCHEME
        value: "http"
      - name: FLUENT_ELASTICSEARCH_USER
        valueFrom:
          secretKeyRef:
            name: elasticsearch-credentials
            key: username
      - name: FLUENT_ELASTICSEARCH_PASSWORD
        valueFrom:
          secretKeyRef:
            name: elasticsearch-credentials
            key: password
      # Prevent buffer overflow with large logs
      - name: FLUENT_CONTAINER_TAIL_EXCLUDE_PATH
        value: '["/var/log/containers/fluentd-*", "/var/log/containers/*logging*", "/var/log/containers/*elasticsearch*"]'
      - name: FLUENT_CONTAINER_TAIL_PARSER_TYPE
        value: "json"
      # Increase buffer limits for large logs
      - name: FLUENT_ELASTICSEARCH_BUFFER_CHUNK_LIMIT_SIZE
        value: "16M"
      # Add plugin for rewrite_tag_filter
      - name: FLUENTD_SYSTEMD_CONF
        value: "disable"
      # Add these environment variables for better stability
      - name: FLUENT_LOG_LEVEL
        value: "error"
      - name: FLUENT_ELASTICSEARCH_RECONNECT_ON_ERROR
        value: "true"
      - name: FLUENT_ELASTICSEARCH_REQUEST_TIMEOUT
        value: "30s"
      - name: FLUENT_ELASTICSEARCH_RELOAD_CONNECTIONS
        value: "true"
