apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-main-config
  namespace: fluent
data:
  fluent.conf: |-
    # Include conf files in config.d directory
    @include config.d/*.conf
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-extra-files
  namespace: fluent
data:
  01_sources.conf: |-
    ## FluentD config file
    ## Sources - logs from fluentbit forwarders
    <source>
        @type forward
        @label @FORWARD
        bind "#{ENV['FLUENTD_FORWARD_BIND'] || '0.0.0.0'}"
        port "#{ENV['FLUENTD_FORWARD_PORT'] || '24224'}"
        # Enabling TLS (commented out for initial setup)
        # <transport tls>
        #     cert_path /etc/fluent/certs/tls.crt
        #     private_key_path /etc/fluent/certs/tls.key
        # </transport>
        # Enabling access security
        <security>
            self_hostname "#{ENV['FLUENTD_FORWARD_SEC_SELFHOSTNAME'] || 'fluentd-aggregator'}"
            shared_key "#{ENV['FLUENT_AGGREGATOR_SHARED_KEY'] || 'sharedkey'}"
        </security>
    </source>
    
    ## Enable Prometheus end point
    <source>
        @type prometheus
        @id in_prometheus
        bind "0.0.0.0"
        port 24231
        metrics_path "/metrics"
    </source>
    <source>
        @type prometheus_monitor
        @id in_prometheus_monitor
    </source>
    <source>
        @type prometheus_output_monitor
        @id in_prometheus_output_monitor
    </source>
  
  02_filters.conf: |-
    ## Filters
    <label @FORWARD>
        # Re-route fluentd logs
        <match kube.var.log.containers.fluentd**>
            @type relabel
            @label @FLUENT_LOG
        </match>
        <match **>
            @type relabel
            @label @DISPATCH
        </match>
    </label>
  
  03_dispatch.conf: |-
    # Discard FLUENTD LOGS
    <label @FLUENT_LOG>
      <match **>
        @type null
        @id ignore_fluent_logs
      </match>
    </label>
    
    # Dispatch logs to different destinations
    <label @DISPATCH>
        # Calculate prometheus metrics
        <filter **>
            @type prometheus
            <metric>
                name fluentd_input_status_num_records_total
                type counter
                desc The total number of incoming records
                <labels>
                tag ${tag}
                hostname ${host}
                </labels>
            </metric>
        </filter>
        # Copy log stream to different outputs
        <match **>
            @type copy
            <store>
                @type relabel
                @label @OUTPUT_ES
            </store>
        </match>
    </label>
  
  04_output.conf: |-
    # Elastic Search Output
    <label @OUTPUT_ES>
        # Setup index name. Index per namespace or per container
        <filter kube.**>
            @type record_transformer
            enable_ruby
            <record>
                index_name ${record['namespace']}
            </record>
        </filter>
        <filter host.**>
            @type record_transformer
            enable_ruby
            <record>
                index_name "host"
            </record>
        </filter>
        
        # Send received logs to elasticsearch
        <match **>
            @type elasticsearch
            @id out_es
            @log_level info
            include_tag_key true
            host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
            port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
            scheme http
            user "#{ENV['FLUENT_ELASTICSEARCH_USER'] || use_default}"
            password "#{ENV['FLUENT_ELASTICSEARCH_PASSWORD'] || use_default}"
            
            # Reload and reconnect options
            reconnect_on_error true
            reload_on_failure true
            reload_connections false
            request_timeout 15s
            log_es_400_reason true
            suppress_type_name true
            sniffer_class_name Fluent::Plugin::ElasticsearchSimpleSniffer
            
            # Do not use logstash format
            logstash_format false
            index_name fluentd-${index_name}
            time_key time
            include_timestamp true
            
            # ILM Settings - WITH ROLLOVER support
            index_date_pattern ""
            enable_ilm true
            ilm_policy_id fluentd-policy
            ilm_policy {"policy":{"phases":{"hot":{"min_age":"0ms","actions":{"rollover":{"max_size":"10gb","max_age":"7d"}}},"warm":{"min_age":"2d","actions":{"shrink":{"number_of_shards":1},"forcemerge":{"max_num_segments":1}}},"delete":{"min_age":"7d","actions":{"delete":{"delete_searchable_snapshot":true}}}}}}
            ilm_policy_overwrite true
            
            # index template
            use_legacy_template false
            template_overwrite true
            template_name fluentd-${index_name}
            template_file "/etc/fluent/template/fluentd-es-template.json"
            # Angepasst f√ºr deine 2-Node Elasticsearch
            customize_template {"<<shard>>": "1","<<replica>>": "1", "<<TAG>>":"${index_name}"}
            remove_keys index_name
            
            <buffer tag, index_name>
                flush_thread_count "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_FLUSH_THREAD_COUNT'] || '8'}"
                flush_interval "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_FLUSH_INTERVAL'] || '5s'}"
                chunk_limit_size "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_CHUNK_LIMIT_SIZE'] || '2M'}"
                queue_limit_length "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_QUEUE_LIMIT_LENGTH'] || '32'}"
                retry_max_interval "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_RETRY_MAX_INTERVAL'] || '30'}"
                retry_forever true
            </buffer>
        </match>
    </label>